# âš¡ LLM â†’ C++ Auto-Optimization Benchmark  
### *Accelerating Python to C++ using GPTâ€‘5, Claude Opus 4.5 & Gemini 2.5 Pro via LiteLLM*

---

## ğŸ“˜ Overview  
This project benchmarks **three LLMs generating optimized C++** from a Python numerical workload.  
The models receive identical system prompts and are evaluated on:

- ğŸ”¥ **Execution Speed vs Python**
- ğŸï¸ **LLM â†’ C++ code compilation success**
- â±ï¸ **LLM generation latency**
- ğŸ’° **Cost per codeâ€‘generation request**

The target script computes Ï€ using a convergent series with **100 million iterations**, testing raw numerical throughput.

---

## ğŸ¯ Objective

1. Convert Python into **highâ€‘performance C++** automatically using LLMs.  
2. Compile and execute the generated C++ and benchmark speedup.  
3. Compare OpenAI, Anthropic and Google models on speed, cost, stability, and latency.

---

## ğŸ§ª Experiment Setup

### Models used
| Key | Model |
|---|---|
| `gpt` | GPTâ€‘5 (OpenAI) |
| `claude` | Claude Opus 4.5 (Anthropic) |
| `gemini` | Gemini 2.5 Pro (Google) |

### System Enforcement Rules  
All models must output pure C++17:

- No `<bits/stdc++.h>`  
- No Markdown or fenced code  
- No OpenMP / pragmas  
- Must compile with clang on **macOS Apple Silicon**

---

## âš™ï¸ Run the Benchmark

```bash
python run.py
```

or inside notebook:

```python
run_for_model_or_all("all", pi, repeats=1)
```

Requires `.env`:

```
OPENAI_API_KEY=...
ANTHROPIC_API_KEY=...
GEMINI_API_KEY=...
```

`LiteLLM` handles routing + cost tracking.

---

## ğŸ”¥ Results

### Python Baseline
```
Result = 3.141592658589
Execution Time = 19.011948s
```

### LLMâ€‘Generated C++ Performance

| Model | Status | Cost ($) | LLM Latency (s) | C++ Runtime (s) | Speedup vs Python |
|---|---|---:|---:|---:|---:|
| GPTâ€‘5 | ok | **0.014804** | 26.002 | **0.284435** | **66.84Ã— faster** |
| Claude 4.5 Opus | ok | 0.043485 | **23.729** | 0.313464 | 60.65Ã— faster |
| Gemini 2.5 Pro | ok | 0.088894 | 73.262 | **0.302568** | 62.84Ã— faster |

---

## ğŸ“ˆ Interpretation & Takeaways

| Model | Best For | Notes |
|---|---|---|
| GPTâ€‘5 | Peak speed + cost efficiency | Best overall speedupâ€‘perâ€‘dollar |
| Claude Opus | Low latency | Fastest LLM response time |
| Gemini 2.5 Pro | Strong output consistency | Slowest LLM latency but still topâ€‘tier C++ speed |

### Summary
- All models generated **valid C++17** and executed successfully.  
- Final C++ binaries ran **60â€“67Ã— faster than Python**.  
- LLM latency dominates total wallâ€‘time (code generation â‰« execution cost).  
- GPT provided the **cheapest + fastest compiled result**, but Claude responded quicker.  

---

## ğŸš€ Next Steps

| Extension | Status |
|---|---|
| Multiâ€‘threaded C++ experiments | ğŸ”œ |
| Prompt optimization for vectorization | ğŸ”œ |
| Add costâ€‘perâ€‘speedup metric | ğŸ”œ |
| Try openâ€‘source models (Qwen, DeepSeek) | ğŸ”œ |

---

## ğŸ Author  
Khashayar Bayati, PhD  
GitHub: **github.com/Khashayarbayati1**

