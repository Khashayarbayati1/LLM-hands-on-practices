{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368da263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and environment setup ---\n",
    "\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion, completion_cost  # LLM call + cost helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e05b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load API keys from .env (do not commit .env to GitHub) ---\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not any([openai_api_key, anthropic_api_key, gemini_api_key]):\n",
    "    raise RuntimeError(\n",
    "        \"No API keys found. Please set at least one of: \"\n",
    "        \"OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model configuration and system prompt for code translation ---\n",
    "\n",
    "models = {\n",
    "    \"gpt\":    \"gpt-5\",\n",
    "    \"claude\": \"anthropic/claude-opus-4-5-20251101\",\n",
    "    \"gemini\": \"gemini/gemini-2.5-pro\", \n",
    "}\n",
    "\n",
    "system_message = (\n",
    "    \"You are an AI code generator. Convert Python code into highly optimized C++ that \"\n",
    "    \"compiles successfully with Clang on macOS (Apple Silicon).\\n\\n\"\n",
    "    \"=== HARD RULES ===\\n\"\n",
    "    \"• Output ONLY valid C++17 source code. No markdown, no backticks, no prose.\\n\"\n",
    "    \"• DO NOT use '#include <bits/stdc++.h>'. Use portable headers like <iostream>, <iomanip>, <cmath>, <vector>, <algorithm>.\\n\"\n",
    "    \"• DO NOT use OpenMP (#include <omp.h> or '#pragma omp ...').\\n\"\n",
    "    \"• DO NOT output shell commands or lines like 'Compile with: ...'.\\n\\n\"\n",
    "    \"=== REQUIREMENTS ===\\n\"\n",
    "    \"• Code must compile with: clang++ -std=c++17 -Ofast -mcpu=native -flto=thin -DNDEBUG\\n\"\n",
    "    \"• Numeric results must match the Python code.\\n\"\n",
    "    \"• Use fast loops, appropriate floating point types, and only standard headers.\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt construction helpers (Python → C++) ---\n",
    "\n",
    "def user_prompt_for(python_code: str) -> str:\n",
    "    return (\n",
    "        \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output \"\n",
    "        \"in the least time. Respond only with valid C++ code; do not include any natural language instructions, \"\n",
    "        \"shell commands, or lines like 'Compile with ...'. \"\n",
    "        \"Use comments sparingly and only inside the C++ file. \"\n",
    "        \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ \"\n",
    "        \"packages such as <iomanip>.\\n\\n\"\n",
    "        f\"{python_code}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def messages_for(python_code: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build the messages payload for the chat completion call.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python_code)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def reasoning_effort_for(model_key: str) -> Optional[str]:\n",
    "    # You want low for GPT + Claude, none for Gemini\n",
    "    if model_key in (\"gpt\", \"claude\"):\n",
    "        return \"low\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File helper: write generated C++ to main.cpp ---\n",
    "\n",
    "def write_output(cpp_code: str, filename: str = \"main.cpp\") -> None:\n",
    "    \"\"\"\n",
    "    Clean up LLM-generated C++ so it compiles on macOS/Clang:\n",
    "      - remove markdown fences\n",
    "      - replace <bits/stdc++.h> with standard headers\n",
    "      - remove OpenMP includes/pragmas\n",
    "      - remove 'Compile with ...' / shell-like junk\n",
    "    \"\"\"\n",
    "    # 1) Remove markdown code fences if any\n",
    "    clean = cpp_code.replace(\"```cpp\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "    lines_out = []\n",
    "    for line in clean.splitlines():\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # 2) Replace GCC-only header with portable includes\n",
    "        if stripped.startswith(\"#include <bits/stdc++.h>\"):\n",
    "            lines_out.append(\"#include <iostream>\")\n",
    "            lines_out.append(\"#include <iomanip>\")\n",
    "            lines_out.append(\"#include <cmath>\")\n",
    "            lines_out.append(\"#include <vector>\")\n",
    "            lines_out.append(\"#include <algorithm>\")\n",
    "            continue\n",
    "\n",
    "        # 3) Drop OpenMP (we're not linking libomp for now)\n",
    "        if \"<omp.h>\" in stripped:\n",
    "            continue\n",
    "        if stripped.startswith(\"#pragma omp\"):\n",
    "            continue\n",
    "\n",
    "        # 4) Drop human instructions / compile hints\n",
    "        if stripped.startswith(\"Compile with:\") or \"clang++\" in stripped:\n",
    "            continue\n",
    "\n",
    "        lines_out.append(line)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM call: return C++ code + cost (USD) + latency (seconds) ---\n",
    "\n",
    "def optimize_llm(python_code: str, model_key: str) -> Tuple[str, float, float]:\n",
    "    if model_key not in models:\n",
    "        raise ValueError(f\"Unknown model key: {model_key}\")\n",
    "\n",
    "    kwargs: Dict[str, object] = {}\n",
    "    effort = reasoning_effort_for(model_key)\n",
    "    if effort is not None:\n",
    "        kwargs[\"reasoning_effort\"] = effort\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    resp = completion(\n",
    "        model=models[model_key],\n",
    "        messages=messages_for(python_code),\n",
    "        **kwargs,\n",
    "    )\n",
    "    latency = time.perf_counter() - start\n",
    "\n",
    "    cpp_code = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    cost = completion_cost(completion_response=resp)\n",
    "\n",
    "    return cpp_code, cost, latency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a902f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline: run the original Python code and time it ---\n",
    "\n",
    "def run_python_and_time(code: str) -> float:\n",
    "    \"\"\"\n",
    "    Execute the Python snippet and return wall-clock execution time in seconds.\n",
    "    The snippet itself may also print its own timing info.\n",
    "    \"\"\"\n",
    "    globals_dict = {\"__builtins__\": __builtins__}\n",
    "    start = time.perf_counter()\n",
    "    exec(code, globals_dict)\n",
    "    end = time.perf_counter()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f45193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C++ compile and run commands (clang++ on main.cpp) ---\n",
    "\n",
    "compile_command = [\n",
    "    \"clang++\", \"main.cpp\", \"-o\", \"main\",\n",
    "    \"-std=c++17\",\n",
    "    \"-Ofast\",\n",
    "    \"-mcpu=native\",\n",
    "    \"-flto=thin\",\n",
    "    \"-fvisibility=hidden\",\n",
    "    \"-DNDEBUG\",\n",
    "    \"-Xpreprocessor\", \"-fopenmp\",\n",
    "    \"-lomp\"\n",
    "]\n",
    "\n",
    "run_command = [\"./main\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b253dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Single-model experiment: Python baseline → LLM → C++ → compile → run ---\n",
    "\n",
    "def run_experiment(\n",
    "    model_key: str,\n",
    "    python_code: str,\n",
    "    repeats: int = 1,\n",
    "    python_time: Optional[float] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For a single model:\n",
    "      1) Use provided python_time if given, otherwise measure it.\n",
    "      2) Use LLM to generate C++ from Python.\n",
    "      3) Compile C++ into ./main.\n",
    "      4) Run the C++ binary 'repeats' times.\n",
    "      5) For each run, print:\n",
    "         - model name\n",
    "         - C++ program output (result + C++ internal timing)\n",
    "         - speedup vs Python baseline\n",
    "         - LLM generation cost and latency\n",
    "    \"\"\"\n",
    "    # 1) Python baseline (measure here only if not provided)\n",
    "    if python_time is None:\n",
    "        python_time = run_python_and_time(python_code)\n",
    "        print(f\"Python baseline execution time: {python_time:.6f} seconds\\n\")\n",
    "\n",
    "    # 2) Generate C++ via LLM\n",
    "    cpp_code, cost_usd, llm_latency = optimize_llm(\n",
    "        python_code=python_code,\n",
    "        model_key=model_key,\n",
    "    )\n",
    "    write_output(cpp_code)\n",
    "\n",
    "    # 3) Compile C++\n",
    "    compile_proc = subprocess.run(\n",
    "        compile_command,\n",
    "        check=False,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "    )\n",
    "    if compile_proc.returncode != 0:\n",
    "        print(f\"Compilation failed for model '{model_key}':\")\n",
    "        print(compile_proc.stderr)\n",
    "        return\n",
    "\n",
    "    # 4) Run C++ binary\n",
    "    for i in range(repeats):\n",
    "        start_cpp = time.perf_counter()\n",
    "        run_proc = subprocess.run(\n",
    "            run_command,\n",
    "            check=False,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        cpp_time = time.perf_counter() - start_cpp\n",
    "\n",
    "        if run_proc.returncode != 0:\n",
    "            print(f\"Run {i + 1} failed for model '{model_key}':\")\n",
    "            print(run_proc.stderr)\n",
    "            continue\n",
    "\n",
    "        improvement = (python_time / cpp_time) if cpp_time > 0 else float(\"inf\")\n",
    "\n",
    "        print(f\"Run {i + 1} output:\")\n",
    "        print(f'Model: \"{model_key}\"')\n",
    "        # C++ program is expected to print \"Result: ...\" and \"Execution Time: ...\"\n",
    "        print(run_proc.stdout.strip())\n",
    "        print(f\"Improvement: {improvement:.2f}x faster than Python\")\n",
    "        print(f\"Cost (LLM generation): ${cost_usd:.6f}\")\n",
    "        print(f\"LLM generation latency: {llm_latency:.3f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb3ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multi-model driver: run one model or all models ---\n",
    "\n",
    "def run_for_model_or_all(model_key: str, python_code: str, repeats: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    If model_key == 'all':\n",
    "        - Run Python baseline once.\n",
    "        - For each model in 'models', generate C++, compile, and run once.\n",
    "    Otherwise:\n",
    "        - Run only the specified model.\n",
    "    \"\"\"\n",
    "    if model_key == \"all\":\n",
    "        # Measure Python baseline once for all models\n",
    "        python_time = run_python_and_time(python_code)\n",
    "        print(f\"Python baseline execution time: {python_time:.6f} seconds\\n\")\n",
    "\n",
    "        for key in models.keys():\n",
    "            print(f\"=== Model: {key} ===\")\n",
    "            try:\n",
    "                run_experiment(\n",
    "                    model_key=key,\n",
    "                    python_code=python_code,\n",
    "                    repeats=repeats,\n",
    "                    python_time=python_time,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error running model '{key}': {e}\\n\")\n",
    "    else:\n",
    "        # Single model; run_experiment will measure Python baseline itself\n",
    "        run_experiment(\n",
    "            model_key=model_key,\n",
    "            python_code=python_code,\n",
    "            repeats=repeats,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Python code: π approximation via series ---\n",
    "\n",
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d5e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 12.104728 seconds\n",
      "Python baseline execution time: 12.105158 seconds\n",
      "\n",
      "=== Model: gpt ===\n",
      "Compilation failed for model 'gpt':\n",
      "main.cpp:1:10: fatal error: 'bits/stdc++.h' file not found\n",
      "    1 | #include <bits/stdc++.h>\n",
      "      |          ^~~~~~~~~~~~~~~\n",
      "1 error generated.\n",
      "\n",
      "=== Model: claude ===\n",
      "Compilation failed for model 'claude':\n",
      "ld: library 'omp' not found\n",
      "clang++: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "\n",
      "=== Model: gemini ===\n",
      "Compilation failed for model 'gemini':\n",
      "ld: library 'omp' not found\n",
      "clang++: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Run the full pipeline: choose one model or 'all' ---\n",
    "\n",
    "# Example 1: run all models once\n",
    "run_for_model_or_all(\"all\", pi, repeats=1)\n",
    "\n",
    "# Example 2: run only GPT once\n",
    "# run_for_model_or_all(\"gpt\", pi, repeats=1)\n",
    "\n",
    "# Example 3: run only Claude or Gemini\n",
    "# run_for_model_or_all(\"claude\", pi, repeats=1)\n",
    "# run_for_model_or_all(\"gemini\", pi, repeats=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_sept25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
