{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "368da263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and environment setup ---\n",
    "\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import subprocess\n",
    "import re\n",
    "import gradio as gr\n",
    "from typing import Dict, Tuple, List, Optional, Any\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion, completion_cost  # LLM call + cost helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e05b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load API keys from .env (do not commit .env to GitHub) ---\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not any([openai_api_key, anthropic_api_key, gemini_api_key]):\n",
    "    raise RuntimeError(\n",
    "        \"No API keys found. Please set at least one of: \"\n",
    "        \"OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, HF_TOKEN.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f42cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model configuration and system prompt for code translation ---\n",
    "\n",
    "models = {\n",
    "    \"gpt\":    \"gpt-5\",\n",
    "    \"claude\": \"anthropic/claude-opus-4-5-20251101\",\n",
    "    \"gemini\": \"gemini/gemini-3-pro-preview\", \n",
    "    \n",
    "    # Open Source via Hugging Face\n",
    "    # \"deepseek_r1\": \"huggingface/together/deepseek-ai/DeepSeek-R1-0528\",\n",
    "    # \"kimi_k2\":     \"huggingface/together/moonshotai/Kimi-K2-Thinking\",\n",
    "    # \"qwen3_235b\":  \"huggingface/fireworks/Qwen/Qwen3-235B-A22B-Thinking-2507\",\n",
    "}\n",
    "\n",
    "system_message = (\n",
    "    \"You are an AI code generator. Convert Python code into highly optimized C++ that \"\n",
    "    \"compiles successfully with Clang on macOS (Apple Silicon).\\n\\n\"\n",
    "    \"=== HARD RULES ===\\n\"\n",
    "    \"• Output ONLY valid C++17 source code. No markdown, no backticks, no prose.\\n\"\n",
    "    \"• DO NOT use '#include <bits/stdc++.h>'. Use portable headers like <iostream>, <iomanip>, <cmath>, <vector>, <algorithm>.\\n\"\n",
    "    \"• DO NOT use OpenMP (#include <omp.h> or '#pragma omp ...').\\n\"\n",
    "    \"• DO NOT output shell commands or lines like 'Compile with: ...'.\\n\\n\"\n",
    "    \"=== REQUIREMENTS ===\\n\"\n",
    "    \"• Code must compile with: clang++ -std=c++17 -Ofast -mcpu=native -flto=thin -DNDEBUG\\n\"\n",
    "    \"• Numeric results must match the Python code.\\n\"\n",
    "    \"• Use fast loops, appropriate floating point types, and only standard headers.\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64be63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt construction helpers (Python → C++) ---\n",
    "\n",
    "def user_prompt_for(python_code: str) -> str:\n",
    "    return (\n",
    "        \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output \"\n",
    "        \"in the least time. Respond only with valid C++ code; do not include any natural language instructions, \"\n",
    "        \"shell commands, or lines like 'Compile with ...'. \"\n",
    "        \"Use comments sparingly and only inside the C++ file. \"\n",
    "        \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ \"\n",
    "        \"packages such as <iomanip>.\\n\\n\"\n",
    "        f\"{python_code}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def messages_for(python_code: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build the messages payload for the chat completion call.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python_code)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def reasoning_effort_for(model_key: str) -> Optional[str]:\n",
    "    # You want low for GPT + Claude, none for Gemini\n",
    "    if model_key in (\"gpt\", \"claude\"):\n",
    "        return \"low\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a0f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File helper: write generated C++ to main.cpp ---\n",
    "\n",
    "results: List[Dict[str, Any]] = [] \n",
    "\n",
    "def strip_think_blocks(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes <think>...</think> blocks.\n",
    "    HANDLES UNCLOSED TAGS: If </think> is missing, removes everything after <think>.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        start = text.find(\"<think>\")\n",
    "        if start == -1:\n",
    "            break\n",
    "        end = text.find(\"</think>\", start)\n",
    "        if end == -1:\n",
    "            # FIX: If tag opens but doesn't close, remove EVERYTHING after start\n",
    "            text = text[:start]\n",
    "            break\n",
    "        text = text[:start] + text[end + len(\"</think>\"):]\n",
    "    return text\n",
    "\n",
    "def write_output(cpp_code: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Clean up LLM-generated C++ so it compiles on macOS/Clang.\n",
    "    \"\"\"\n",
    "    # 1) Try extracting code block via Regex from RAW output first.\n",
    "    #    This handles cases where <think> is unclosed but code follows in fences.\n",
    "    match = re.search(r\"```(cpp|c\\+\\+)?(.*?)```\", cpp_code, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        clean = match.group(2) # The content inside the fences\n",
    "    else:\n",
    "        # 2) Fallback: If no fences, try stripping <think> blocks.\n",
    "        clean = strip_think_blocks(cpp_code)\n",
    "        \n",
    "        # 3) If stripping resulted in empty string (e.g. unclosed tag), \n",
    "        #    or just prose, try finding the start of C++ code manually.\n",
    "        if not clean.strip():\n",
    "            # Try finding the first #include as a heuristic anchor\n",
    "            idx = cpp_code.find(\"#include\")\n",
    "            if idx != -1:\n",
    "                clean = cpp_code[idx:]\n",
    "            else:\n",
    "                # If all else fails, keep original to avoid empty file errors\n",
    "                clean = cpp_code\n",
    "        \n",
    "        # Remove manual fences if they survived\n",
    "        clean = clean.replace(\"```cpp\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "    lines_out = []\n",
    "    for line in clean.splitlines():\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # 4) Replace GCC-only header with portable includes\n",
    "        if stripped.startswith(\"#include <bits/stdc++.h>\"):\n",
    "            lines_out.append(\"#include <iostream>\")\n",
    "            lines_out.append(\"#include <iomanip>\")\n",
    "            lines_out.append(\"#include <cmath>\")\n",
    "            lines_out.append(\"#include <vector>\")\n",
    "            lines_out.append(\"#include <algorithm>\")\n",
    "            continue\n",
    "\n",
    "        # 5) Drop OpenMP \n",
    "        if \"<omp.h>\" in stripped:\n",
    "            continue\n",
    "        if stripped.startswith(\"#pragma omp\"):\n",
    "            continue\n",
    "\n",
    "        # 6) Drop human instructions / compile hints\n",
    "        if stripped.startswith(\"Compile with:\") or \"clang++\" in stripped:\n",
    "            continue\n",
    "\n",
    "        lines_out.append(line)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dfe2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM call: return C++ code + cost (USD) + latency (seconds) ---\n",
    "\n",
    "def optimize_llm(python_code: str, model_key: str) -> Tuple[str, float, float]:\n",
    "    if model_key not in models:\n",
    "        raise ValueError(f\"Unknown model key: {model_key}\")\n",
    "\n",
    "    kwargs: Dict[str, object] = {}\n",
    "    effort = reasoning_effort_for(model_key)\n",
    "    if effort is not None:\n",
    "        kwargs[\"reasoning_effort\"] = effort\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    resp = completion(\n",
    "        model=models[model_key],\n",
    "        messages=messages_for(python_code),\n",
    "        **kwargs,\n",
    "    )\n",
    "    latency = time.perf_counter() - start\n",
    "\n",
    "    cpp_code = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    try:\n",
    "        cost = completion_cost(completion_response=resp)\n",
    "    except Exception:\n",
    "        cost = None\n",
    "\n",
    "    return cpp_code, cost, latency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a902f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline: run the original Python code and time it ---\n",
    "\n",
    "def run_python_and_time(code: str) -> float:\n",
    "    \"\"\"\n",
    "    Execute the Python snippet and return wall-clock execution time in seconds.\n",
    "    The snippet itself may also print its own timing info.\n",
    "    \"\"\"\n",
    "    globals_dict = {\"__builtins__\": __builtins__}\n",
    "    start = time.perf_counter()\n",
    "    exec(code, globals_dict)\n",
    "    end = time.perf_counter()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f45193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C++ compile and run commands (clang++ on main.cpp) ---\n",
    "\n",
    "compile_command = [\n",
    "    \"clang++\", \"main.cpp\", \"-o\", \"main\",\n",
    "    \"-std=c++17\",\n",
    "    \"-Ofast\",\n",
    "    \"-mcpu=native\",\n",
    "    \"-flto=thin\",\n",
    "    \"-fvisibility=hidden\",\n",
    "    \"-DNDEBUG\",\n",
    "    \"-Xpreprocessor\", \"-fopenmp\",\n",
    "]\n",
    "\n",
    "run_command = [\"./main\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b253dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Single-model experiment: Python baseline → LLM → C++ → compile → run ---\n",
    "\n",
    "def run_experiment(\n",
    "    model_key: str,\n",
    "    python_code: str,\n",
    "    repeats: int = 1,\n",
    "    python_time: Optional[float] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For a single model:\n",
    "      1) Use provided python_time if given, otherwise measure it.\n",
    "      2) Use LLM to generate C++ from Python.\n",
    "      3) Compile C++ into ./main.\n",
    "      4) Run the C++ binary 'repeats' times.\n",
    "      5) Append a summary entry into the global `results` list with:\n",
    "         - model, status, cpp_time, speedup, cost, latency, error (if any).\n",
    "    \"\"\"\n",
    "    global results\n",
    "\n",
    "    # 1) Python baseline (measure here only if not provided)\n",
    "    if python_time is None:\n",
    "        python_time = run_python_and_time(python_code)\n",
    "        print(f\"Python baseline execution time: {python_time:.6f} seconds\\n\")\n",
    "\n",
    "    # 2) Generate C++ via LLM\n",
    "    try:\n",
    "        cpp_code, cost_usd, llm_latency = optimize_llm(\n",
    "            python_code=python_code,\n",
    "            model_key=model_key,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"LLM generation failed for model '{model_key}': {e}\")\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"status\": \"llm_failed\",\n",
    "            \"cpp_time\": None,\n",
    "            \"speedup\": None,\n",
    "            \"cost\": None,\n",
    "            \"latency\": None,\n",
    "            \"error\": str(e)[:200],\n",
    "        })\n",
    "        return\n",
    "\n",
    "    cpp_filename = f\"{model_key}.cpp\"\n",
    "    write_output(cpp_code, filename=cpp_filename)\n",
    "\n",
    "    # 3) Compile C++ with the model name\n",
    "    cpp_filename = f\"{model_key}.cpp\"\n",
    "    write_output(cpp_code, filename=cpp_filename)\n",
    "    \n",
    "    compile_cmd = compile_command.copy()\n",
    "    compile_cmd[1] = cpp_filename     # replace 'main.cpp'\n",
    "    compile_cmd[3] = model_key        # output binary: gpt, claude, gemini\n",
    "\n",
    "    compile_proc = subprocess.run(\n",
    "        compile_cmd, \n",
    "        check=False, \n",
    "        text=True, \n",
    "        capture_output=True\n",
    "    )\n",
    "\n",
    "    if compile_proc.returncode != 0:\n",
    "        print(f\"Compilation failed for model '{model_key}':\")\n",
    "        print(compile_proc.stderr)\n",
    "\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"status\": \"compile_failed\",\n",
    "            \"cpp_time\": None,\n",
    "            \"speedup\": None,\n",
    "            \"cost\": cost_usd,\n",
    "            \"latency\": llm_latency,\n",
    "            \"error\": compile_proc.stderr[:200],\n",
    "        })\n",
    "        return\n",
    "\n",
    "    # 4) Run C++ binary (possibly multiple times)\n",
    "    best_cpp_time: Optional[float] = None\n",
    "    best_speedup: Optional[float] = None\n",
    "\n",
    "    for i in range(repeats):\n",
    "        start_cpp = time.perf_counter()\n",
    "        run_proc = subprocess.run(\n",
    "            run_command,\n",
    "            check=False,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        cpp_time = time.perf_counter() - start_cpp\n",
    "\n",
    "        if run_proc.returncode != 0:\n",
    "            print(f\"Run {i + 1} failed for model '{model_key}':\")\n",
    "            print(run_proc.stderr)\n",
    "            continue\n",
    "\n",
    "        improvement = (python_time / cpp_time) if cpp_time > 0 else float(\"inf\")\n",
    "\n",
    "        # Print per-run detail\n",
    "        print(f\"Run {i + 1} output:\")\n",
    "        print(f'Model: \"{model_key}\"')\n",
    "        print(run_proc.stdout.strip())\n",
    "        print(f\"Improvement: {improvement:.2f}x faster than Python\")\n",
    "        print(f\"Cost (LLM generation): ${cost_usd:.6f}\")\n",
    "        print(f\"LLM generation latency: {llm_latency:.3f} seconds\\n\")\n",
    "\n",
    "        # Track best run\n",
    "        if best_cpp_time is None or cpp_time < best_cpp_time:\n",
    "            best_cpp_time = cpp_time\n",
    "            best_speedup = improvement\n",
    "\n",
    "    # 5) Record result summary\n",
    "    if best_cpp_time is None:\n",
    "        # All runs failed at runtime\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"status\": \"run_failed\",\n",
    "            \"cpp_time\": None,\n",
    "            \"speedup\": None,\n",
    "            \"cost\": cost_usd,\n",
    "            \"latency\": llm_latency,\n",
    "            \"error\": \"Runtime failure in all runs\",\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"status\": \"ok\",\n",
    "            \"cpp_time\": best_cpp_time,\n",
    "            \"speedup\": best_speedup,\n",
    "            \"cost\": cost_usd,\n",
    "            \"latency\": llm_latency,\n",
    "            \"error\": None,\n",
    "        })\n",
    "\n",
    "def print_results_table() -> None:\n",
    "    \"\"\"\n",
    "    Print a simple summary table from the global `results` list.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"\\nNo results to summarize.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Summary Table ===\")\n",
    "    print(f\"{'Model':<10}{'Status':<15}{'Cost($)':<12}{'Latency(s)':<14}{'Cpp(s)':<12}{'Speedup(x)':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for r in results:\n",
    "        model   = r.get(\"model\", \"\")\n",
    "        status  = r.get(\"status\", \"\")\n",
    "        cost    = r.get(\"cost\", None)\n",
    "        lat     = r.get(\"latency\", None)\n",
    "        cpp_t   = r.get(\"cpp_time\", None)\n",
    "        speedup = r.get(\"speedup\", None)\n",
    "\n",
    "        cost_str    = f\"{cost:.6f}\" if cost is not None else \"N/A\"\n",
    "        lat_str     = f\"{lat:.3f}\" if lat is not None else \"N/A\"\n",
    "        cpp_str     = f\"{cpp_t:.6f}\" if cpp_t is not None else \"N/A\"\n",
    "        speedup_str = f\"{speedup:.2f}\" if speedup is not None else \"N/A\"\n",
    "\n",
    "        print(f\"{model:<10}{status:<15}{cost_str:<12}{lat_str:<14}{cpp_str:<12}{speedup_str:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeb3ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multi-model driver: run one model or all models ---\n",
    "\n",
    "def run_for_model_or_all(model_key: str, python_code: str, repeats: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    If model_key == 'all':\n",
    "        - Run Python baseline once.\n",
    "        - For each model in 'models', generate C++, compile, and run.\n",
    "    Otherwise:\n",
    "        - Run only the specified model.\n",
    "    At the end, print a summary table from `results`.\n",
    "    \"\"\"\n",
    "    global results\n",
    "    results.clear()  # reset from any previous experiment\n",
    "\n",
    "    if model_key == \"all\":\n",
    "        # Measure Python baseline once for all models\n",
    "        python_time = run_python_and_time(python_code)\n",
    "        print(f\"Python baseline execution time: {python_time:.6f} seconds\\n\")\n",
    "\n",
    "        for key in models.keys():\n",
    "            print(f\"=== Model: {key} ===\")\n",
    "            try:\n",
    "                run_experiment(\n",
    "                    model_key=key,\n",
    "                    python_code=python_code,\n",
    "                    repeats=repeats,\n",
    "                    python_time=python_time,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for model '{key}': {e}\")\n",
    "                results.append({\n",
    "                    \"model\": key,\n",
    "                    \"status\": \"unexpected_error\",\n",
    "                    \"cpp_time\": None,\n",
    "                    \"speedup\": None,\n",
    "                    \"cost\": None,\n",
    "                    \"latency\": None,\n",
    "                    \"error\": str(e)[:200],\n",
    "                })\n",
    "    else:\n",
    "        # Single model; run_experiment will measure Python baseline itself\n",
    "        print(f\"=== Model: {model_key} ===\")\n",
    "        run_experiment(\n",
    "            model_key=model_key,\n",
    "            python_code=python_code,\n",
    "            repeats=repeats,\n",
    "        )\n",
    "\n",
    "    # Print summary for all attempted models\n",
    "    print_results_table()\n",
    "    \n",
    "def run_for_model_or_all_returning_strings(model_key: str, python_code: str, repeats: int = 1):\n",
    "    global results\n",
    "    results.clear()\n",
    "    \n",
    "    buffer = io.StringIO()\n",
    "    with redirect_stdout(buffer):\n",
    "        run_for_model_or_all(model_key, python_code, repeats=repeats)\n",
    "        \n",
    "    full_log = buffer.getvalue()\n",
    "    \n",
    "    # Now build summary separately (instead of priniting it)\n",
    "    summary_buffer = io.StringIO()\n",
    "    with redirect_stdout(summary_buffer):\n",
    "        print_results_table()\n",
    "    summary_table = summary_buffer.getvalue()\n",
    "    \n",
    "    return full_log, summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Python code: π approximation via series ---\n",
    "\n",
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_run(model_key: str, python_code: str, repeats: int):\n",
    "    full_log, summary_table = run_for_model_or_all_returning_strings(\n",
    "        model_key=model_key.strip(),    # to avoid \"all \" vs \"all\" bug.\n",
    "        python_code=python_code,\n",
    "        repeats=int(repeats)            # in case Gradio gives it as float\n",
    "    )\n",
    "    return full_log, summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57ed8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Python → C++ Optimizer\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        \n",
    "        # Python → C++ Optimizer\n",
    "        \n",
    "        Paste Python code, choose an LLM backend, and benchmark the generated C++\n",
    "        against the Python baseline. This tool calls multiple models via LiteLLM\n",
    "        and compiles the result with `clang++`.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            model_input = gr.Radio(\n",
    "            choices=[\"gpt\", \"claude\", \"gemini\", \"all\"],\n",
    "            value=\"gpt\",\n",
    "            label=\"Model\",\n",
    "            info=\"Use 'all' to compare all configured models.\"\n",
    "            )\n",
    "         \n",
    "            repeats_input = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=5,\n",
    "                step=1,\n",
    "                value=1,\n",
    "                label=\"C++ runs (repeats)\",\n",
    "                info=\"Each run will execute the compiled C++ binary and measure runtime.\",\n",
    "            )    \n",
    "            \n",
    "            run_button = gr.Button(\"Run optimization\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            code_input = gr.Code(\n",
    "                value=pi,\n",
    "                lines=20,\n",
    "                label=\"Python code\",\n",
    "                language=\"python\",\n",
    "            )     \n",
    "        \n",
    "    with gr.Tab(\"Log\"):\n",
    "        log_output = gr.Textbox(\n",
    "            lines=18,\n",
    "            label=\"Detailed log\",\n",
    "            interactive=False,\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"Summary\"):\n",
    "        summary_output = gr.Textbox(\n",
    "        lines=10,\n",
    "        label=\"summary_table\",\n",
    "        show_label=True,\n",
    "        interactive=False,\n",
    "        )\n",
    "        \n",
    "    with gr.Accordion(\"Examples\", open=False):\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"gpt\", pi, 1],\n",
    "                [\"all\", pi, 1],\n",
    "            ],\n",
    "            inputs=[model_input, code_input, repeats_input],\n",
    "            label=\"Quick examples\", \n",
    "        )\n",
    "        \n",
    "    run_button.click(\n",
    "        fn=gradio_run,\n",
    "        inputs=[model_input, code_input, repeats_input],\n",
    "        outputs=[log_output, summary_output],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09d5e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8812174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_sept25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
