{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368da263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and environment setup ---\n",
    "\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from typing import Dict, Tuple, List, Optional, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion, completion_cost  # LLM call + cost helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e05b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load API keys from .env (do not commit .env to GitHub) ---\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not any([openai_api_key, anthropic_api_key, gemini_api_key]):\n",
    "    raise RuntimeError(\n",
    "        \"No API keys found. Please set at least one of: \"\n",
    "        \"OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f42cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model configuration and system prompt for code translation ---\n",
    "\n",
    "models = {\n",
    "    \"gpt\":    \"gpt-5\",\n",
    "    \"claude\": \"anthropic/claude-opus-4-5-20251101\",\n",
    "    \"gemini\": \"gemini/gemini-2.5-pro\", \n",
    "}\n",
    "\n",
    "system_message = (\n",
    "    \"You are an AI code generator. Convert Python code into highly optimized C++ that \"\n",
    "    \"compiles successfully with Clang on macOS (Apple Silicon).\\n\\n\"\n",
    "    \"=== HARD RULES ===\\n\"\n",
    "    \"• Output ONLY valid C++17 source code. No markdown, no backticks, no prose.\\n\"\n",
    "    \"• DO NOT use '#include <bits/stdc++.h>'. Use portable headers like <iostream>, <iomanip>, <cmath>, <vector>, <algorithm>.\\n\"\n",
    "    \"• DO NOT use OpenMP (#include <omp.h> or '#pragma omp ...').\\n\"\n",
    "    \"• DO NOT output shell commands or lines like 'Compile with: ...'.\\n\\n\"\n",
    "    \"=== REQUIREMENTS ===\\n\"\n",
    "    \"• Code must compile with: clang++ -std=c++17 -Ofast -mcpu=native -flto=thin -DNDEBUG\\n\"\n",
    "    \"• Numeric results must match the Python code.\\n\"\n",
    "    \"• Use fast loops, appropriate floating point types, and only standard headers.\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64be63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt construction helpers (Python → C++) ---\n",
    "\n",
    "def user_prompt_for(python_code: str) -> str:\n",
    "    return (\n",
    "        \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output \"\n",
    "        \"in the least time. Respond only with valid C++ code; do not include any natural language instructions, \"\n",
    "        \"shell commands, or lines like 'Compile with ...'. \"\n",
    "        \"Use comments sparingly and only inside the C++ file. \"\n",
    "        \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ \"\n",
    "        \"packages such as <iomanip>.\\n\\n\"\n",
    "        f\"{python_code}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def messages_for(python_code: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build the messages payload for the chat completion call.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python_code)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def reasoning_effort_for(model_key: str) -> Optional[str]:\n",
    "    # You want low for GPT + Claude, none for Gemini\n",
    "    if model_key in (\"gpt\", \"claude\"):\n",
    "        return \"low\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a0f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File helper: write generated C++ to main.cpp ---\n",
    "\n",
    "results: List[Dict[str, Any]] = [] \n",
    "\n",
    "def write_output(cpp_code: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Clean up LLM-generated C++ so it compiles on macOS/Clang:\n",
    "      - remove markdown fences\n",
    "      - replace <bits/stdc++.h> with standard headers\n",
    "      - remove OpenMP includes/pragmas\n",
    "      - remove 'Compile with ...' / shell-like junk\n",
    "    \"\"\"\n",
    "    # 1) Remove markdown code fences if any\n",
    "    clean = cpp_code.replace(\"```cpp\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "    lines_out = []\n",
    "    for line in clean.splitlines():\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # 2) Replace GCC-only header with portable includes\n",
    "        if stripped.startswith(\"#include <bits/stdc++.h>\"):\n",
    "            lines_out.append(\"#include <iostream>\")\n",
    "            lines_out.append(\"#include <iomanip>\")\n",
    "            lines_out.append(\"#include <cmath>\")\n",
    "            lines_out.append(\"#include <vector>\")\n",
    "            lines_out.append(\"#include <algorithm>\")\n",
    "            continue\n",
    "\n",
    "        # 3) Drop OpenMP \n",
    "        if \"<omp.h>\" in stripped:\n",
    "            continue\n",
    "        if stripped.startswith(\"#pragma omp\"):\n",
    "            continue\n",
    "\n",
    "        # 4) Drop human instructions / compile hints\n",
    "        if stripped.startswith(\"Compile with:\") or \"clang++\" in stripped:\n",
    "            continue\n",
    "\n",
    "        lines_out.append(line)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dfe2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM call: return C++ code + cost (USD) + latency (seconds) ---\n",
    "\n",
    "def optimize_llm(python_code: str, model_key: str) -> Tuple[str, float, float]:\n",
    "    if model_key not in models:\n",
    "        raise ValueError(f\"Unknown model key: {model_key}\")\n",
    "\n",
    "    kwargs: Dict[str, object] = {}\n",
    "    effort = reasoning_effort_for(model_key)\n",
    "    if effort is not None:\n",
    "        kwargs[\"reasoning_effort\"] = effort\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    resp = completion(\n",
    "        model=models[model_key],\n",
    "        messages=messages_for(python_code),\n",
    "        **kwargs,\n",
    "    )\n",
    "    latency = time.perf_counter() - start\n",
    "\n",
    "    cpp_code = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    cost = completion_cost(completion_response=resp)\n",
    "\n",
    "    return cpp_code, cost, latency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a902f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline: run the original Python code and time it ---\n",
    "\n",
    "def run_python_and_time(code: str) -> float:\n",
    "    \"\"\"\n",
    "    Execute the Python snippet and return wall-clock execution time in seconds.\n",
    "    The snippet itself may also print its own timing info.\n",
    "    \"\"\"\n",
    "    globals_dict = {\"__builtins__\": __builtins__}\n",
    "    start = time.perf_counter()\n",
    "    exec(code, globals_dict)\n",
    "    end = time.perf_counter()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f45193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C++ compile and run commands (clang++ on main.cpp) ---\n",
    "\n",
    "compile_command = [\n",
    "    \"clang++\", \"main.cpp\", \"-o\", \"main\",\n",
    "    \"-std=c++17\",\n",
    "    \"-Ofast\",\n",
    "    \"-mcpu=native\",\n",
    "    \"-flto=thin\",\n",
    "    \"-fvisibility=hidden\",\n",
    "    \"-DNDEBUG\",\n",
    "    \"-Xpreprocessor\", \"-fopenmp\",\n",
    "]\n",
    "\n",
    "run_command = [\"./main\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b253dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Single-model experiment: Python baseline → LLM → C++ → compile → run ---\n",
    "\n",
    "def run_experiment(\n",
    "    model_key: str,\n",
    "    python_code: str,\n",
    "    repeats: int = 1,\n",
    "    python_time: Optional[float] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For a single model:\n",
    "      1) Use provided python_time if given, otherwise measure it.\n",
    "      2) Use LLM to generate C++ from Python.\n",
    "      3) Compile C++ into ./main.\n",
    "      4) Run the C++ binary 'repeats' times.\n",
    "      5) Append a summary entry into the global `results` list with:\n",
    "         - model, status, cpp_time, speedup, cost, latency, error (if any).\n",
    "    \"\"\"\n",
    "    global results\n",
    "\n",
    "    # 1) Python baseline (measure here only if not provided)\n",
    "    if python_time is None:\n",
    "        python_time = run_python_and_time(python_code)\n",
    "        print(f\"Python baseline execution time: {python_time:.6f} seconds\\n\")\n",
    "\n",
    "    # 2) Generate C++ via LLM\n",
    "    try:\n",
    "        cpp_code, cost_usd, llm_latency = optimize_llm(\n",
    "            python_code=python_code,\n",
    "            model_key=model_key,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"LLM generation failed for model '{model_key}': {e}\")\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"status\": \"llm_failed\",\n",
    "            \"cpp_time\": None,\n",
    "            \"speedup\": None,\n",
    "            \"cost\": None,\n",
    "            \"latency\": None,\n",
    "            \"error\": str(e)[:200],\n",
    "        })\n",
    "        return\n",
    "\n",
    "    cpp_filename = f\"{model_key}.cpp\"\n",
    "    write_output(cpp_code, filename=cpp_filename)\n",
    "\n",
    "    # 3) Compile C++ with the model name\n",
    "    cpp_filename = f\"{model_key}.cpp\"\n",
    "    write_output(cpp_code, filename=cpp_filename)\n",
    "    \n",
    "    compile_cmd = compile_command.copy()\n",
    "    compile_cmd[1] = cpp_filename     # replace 'main.cpp'\n",
    "    compile_cmd[3] = model_key        # output binary: gpt, claude, gemini\n",
    "\n",
    "    compile_proc = subprocess.run(\n",
    "        compile_cmd, \n",
    "        check=False, \n",
    "        text=True, \n",
    "        capture_output=True\n",
    "    )\n",
    "\n",
    "    if compile_proc.returncode != 0:\n",
    "        print(f\"Compilation failed for model '{model_key}':\")\n",
    "        print(compile_proc.stderr)\n",
    "\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"status\": \"compile_failed\",\n",
    "            \"cpp_time\": None,\n",
    "            \"speedup\": None,\n",
    "            \"cost\": cost_usd,\n",
    "            \"latency\": llm_latency,\n",
    "            \"error\": compile_proc.stderr[:200],\n",
    "        })\n",
    "        return\n",
    "\n",
    "    # 4) Run C++ binary (possibly multiple times)\n",
    "    best_cpp_time: Optional[float] = None\n",
    "    best_speedup: Optional[float] = None\n",
    "\n",
    "    for i in range(repeats):\n",
    "        start_cpp = time.perf_counter()\n",
    "        run_proc = subprocess.run(\n",
    "            run_command,\n",
    "            check=False,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        cpp_time = time.perf_counter() - start_cpp\n",
    "\n",
    "        if run_proc.returncode != 0:\n",
    "            print(f\"Run {i + 1} failed for model '{model_key}':\")\n",
    "            print(run_proc.stderr)\n",
    "            continue\n",
    "\n",
    "        improvement = (python_time / cpp_time) if cpp_time > 0 else float(\"inf\")\n",
    "\n",
    "        # Print per-run detail\n",
    "        print(f\"Run {i + 1} output:\")\n",
    "        print(f'Model: \"{model_key}\"')\n",
    "        print(run_proc.stdout.strip())\n",
    "        print(f\"Improvement: {improvement:.2f}x faster than Python\")\n",
    "        print(f\"Cost (LLM generation): ${cost_usd:.6f}\")\n",
    "        print(f\"LLM generation latency: {llm_latency:.3f} seconds\\n\")\n",
    "\n",
    "        # Track best run\n",
    "        if best_cpp_time is None or cpp_time < best_cpp_time:\n",
    "            best_cpp_time = cpp_time\n",
    "            best_speedup = improvement\n",
    "\n",
    "    # 5) Record result summary\n",
    "    if best_cpp_time is None:\n",
    "        # All runs failed at runtime\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"status\": \"run_failed\",\n",
    "            \"cpp_time\": None,\n",
    "            \"speedup\": None,\n",
    "            \"cost\": cost_usd,\n",
    "            \"latency\": llm_latency,\n",
    "            \"error\": \"Runtime failure in all runs\",\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"status\": \"ok\",\n",
    "            \"cpp_time\": best_cpp_time,\n",
    "            \"speedup\": best_speedup,\n",
    "            \"cost\": cost_usd,\n",
    "            \"latency\": llm_latency,\n",
    "            \"error\": None,\n",
    "        })\n",
    "\n",
    "def print_results_table() -> None:\n",
    "    \"\"\"\n",
    "    Print a simple summary table from the global `results` list.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"\\nNo results to summarize.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Summary Table ===\")\n",
    "    print(f\"{'Model':<10}{'Status':<15}{'Cost($)':<12}{'Latency(s)':<14}{'Cpp(s)':<12}{'Speedup(x)':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for r in results:\n",
    "        model   = r.get(\"model\", \"\")\n",
    "        status  = r.get(\"status\", \"\")\n",
    "        cost    = r.get(\"cost\", None)\n",
    "        lat     = r.get(\"latency\", None)\n",
    "        cpp_t   = r.get(\"cpp_time\", None)\n",
    "        speedup = r.get(\"speedup\", None)\n",
    "\n",
    "        cost_str    = f\"{cost:.6f}\" if cost is not None else \"N/A\"\n",
    "        lat_str     = f\"{lat:.3f}\" if lat is not None else \"N/A\"\n",
    "        cpp_str     = f\"{cpp_t:.6f}\" if cpp_t is not None else \"N/A\"\n",
    "        speedup_str = f\"{speedup:.2f}\" if speedup is not None else \"N/A\"\n",
    "\n",
    "        print(f\"{model:<10}{status:<15}{cost_str:<12}{lat_str:<14}{cpp_str:<12}{speedup_str:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb3ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multi-model driver: run one model or all models ---\n",
    "\n",
    "def run_for_model_or_all(model_key: str, python_code: str, repeats: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    If model_key == 'all':\n",
    "        - Run Python baseline once.\n",
    "        - For each model in 'models', generate C++, compile, and run.\n",
    "    Otherwise:\n",
    "        - Run only the specified model.\n",
    "    At the end, print a summary table from `results`.\n",
    "    \"\"\"\n",
    "    global results\n",
    "    results.clear()  # reset from any previous experiment\n",
    "\n",
    "    if model_key == \"all\":\n",
    "        # Measure Python baseline once for all models\n",
    "        python_time = run_python_and_time(python_code)\n",
    "        print(f\"Python baseline execution time: {python_time:.6f} seconds\\n\")\n",
    "\n",
    "        for key in models.keys():\n",
    "            print(f\"=== Model: {key} ===\")\n",
    "            try:\n",
    "                run_experiment(\n",
    "                    model_key=key,\n",
    "                    python_code=python_code,\n",
    "                    repeats=repeats,\n",
    "                    python_time=python_time,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for model '{key}': {e}\")\n",
    "                results.append({\n",
    "                    \"model\": key,\n",
    "                    \"status\": \"unexpected_error\",\n",
    "                    \"cpp_time\": None,\n",
    "                    \"speedup\": None,\n",
    "                    \"cost\": None,\n",
    "                    \"latency\": None,\n",
    "                    \"error\": str(e)[:200],\n",
    "                })\n",
    "    else:\n",
    "        # Single model; run_experiment will measure Python baseline itself\n",
    "        print(f\"=== Model: {model_key} ===\")\n",
    "        run_experiment(\n",
    "            model_key=model_key,\n",
    "            python_code=python_code,\n",
    "            repeats=repeats,\n",
    "        )\n",
    "\n",
    "    # Print summary for all attempted models\n",
    "    print_results_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Python code: π approximation via series ---\n",
    "\n",
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d5e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 21.725370 seconds\n",
      "Python baseline execution time: 21.714268 seconds\n",
      "\n",
      "=== Model: gpt ===\n",
      "Run 1 output:\n",
      "Model: \"gpt\"\n",
      "Result: 3.141592659092\n",
      "Execution Time: 0.051358 seconds\n",
      "Improvement: 355.79x faster than Python\n",
      "Cost (LLM generation): $0.012874\n",
      "LLM generation latency: 22.992 seconds\n",
      "\n",
      "=== Model: claude ===\n",
      "Run 1 output:\n",
      "Model: \"claude\"\n",
      "Result: 3.141592659092\n",
      "Execution Time: 0.051650 seconds\n",
      "Improvement: 386.14x faster than Python\n",
      "Cost (LLM generation): $0.034185\n",
      "LLM generation latency: 15.259 seconds\n",
      "\n",
      "=== Model: gemini ===\n",
      "Run 1 output:\n",
      "Model: \"gemini\"\n",
      "Result: 3.141592659092\n",
      "Execution Time: 0.052302 seconds\n",
      "Improvement: 383.22x faster than Python\n",
      "Cost (LLM generation): $0.077374\n",
      "LLM generation latency: 56.276 seconds\n",
      "\n",
      "\n",
      "=== Summary Table ===\n",
      "Model     Status         Cost($)     Latency(s)    Cpp(s)      Speedup(x)\n",
      "----------------------------------------------------------------------\n",
      "gpt       ok             0.012874    22.992        0.061031    355.79    \n",
      "claude    ok             0.034185    15.259        0.056234    386.14    \n",
      "gemini    ok             0.077374    56.276        0.056662    383.22    \n"
     ]
    }
   ],
   "source": [
    "# --- Run the full pipeline: choose one model or 'all' ---\n",
    "\n",
    "# Example 1: run all models once\n",
    "run_for_model_or_all(\"all\", pi, repeats=1)\n",
    "\n",
    "# Example 2: run only GPT once\n",
    "# run_for_model_or_all(\"gpt\", pi, repeats=1)\n",
    "\n",
    "# Example 3: run only Claude or Gemini\n",
    "# run_for_model_or_all(\"claude\", pi, repeats=1)\n",
    "# run_for_model_or_all(\"gemini\", pi, repeats=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_sept25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
